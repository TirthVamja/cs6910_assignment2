{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import Sequential, Module, ReLU, Conv2d, Linear, MaxPool2d, LogSoftmax, NLLLoss, Dropout, BatchNorm2d, LeakyReLU, GELU, SELU, Mish, CrossEntropyLoss\n",
    "from torchvision import transforms, datasets, models\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import flatten, float, no_grad\n",
    "from torch.optim import Adam\n",
    "import torch\n",
    "import wandb\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(param, type):\n",
    "    if(type.lower() == 'train'):\n",
    "        transform = transforms.Compose([\n",
    "            transforms.Resize(256),\n",
    "            # transforms.RandomResizedCrop(size=256, scale=(0.8, 1.0)),\n",
    "            transforms.RandomHorizontalFlip(p=0.3),\n",
    "            transforms.RandomRotation(degrees=12),\n",
    "            transforms.ColorJitter(),\n",
    "            transforms.CenterCrop(224),\n",
    "            transforms.ToTensor(), \n",
    "            transforms.Normalize(mean = [0.485, 0.456, 0.406], std = [0.229, 0.224, 0.225])  \n",
    "        ])\n",
    "        # print(\"hi\")\n",
    "        # print(param)\n",
    "        tdataset = datasets.ImageFolder(root=param['train_data_dir'], transform=transform)\n",
    "        total = len(tdataset)\n",
    "        train_sample = math.ceil(total*(0.8))\n",
    "        val_sample = total-train_sample\n",
    "        # print(total, train_sample, val_sample)\n",
    "        train_dataset, validation_dataset = torch.utils.data.random_split(tdataset, [train_sample, val_sample])\n",
    "        train_dataloader = DataLoader(train_dataset, batch_size=param['batch_size'], shuffle=True)\n",
    "        validation_dataloader = DataLoader(validation_dataset, batch_size=param['batch_size'], shuffle=False)\n",
    "        return train_dataloader, validation_dataloader\n",
    "    \n",
    "    else:\n",
    "        transform = transforms.Compose([\n",
    "            transforms.RandomResizedCrop(size=256, scale=(0.8, 1.0)),\n",
    "            transforms.CenterCrop(size=224),\n",
    "            transforms.ToTensor(), \n",
    "            transforms.Normalize(mean = [0.485, 0.456, 0.406], std = [0.229, 0.224, 0.225])  \n",
    "        ])\n",
    "        test_dataset = datasets.ImageFolder(root=param['test_data_dir'], transform=transform)\n",
    "        test_dataloader = DataLoader(test_dataset, batch_size=param['batch_size'])\n",
    "        return test_dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer Learning using VGG16 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "\n",
    "    wandb.init()\n",
    "    param = wandb.config\n",
    "    wandb.run.name = f'vgg16_strategy_{param.strategy}_batchSz_{param.batch_size}_epochs_{param.epochs}'\n",
    "\n",
    "    # param = {\n",
    "    #     \"batch_size\": 32,\n",
    "    #     \"epochs\": 5,\n",
    "    #     \"train_data_dir\": \"./data/train\",\n",
    "    #     \"test_data_dir\": \"./data/val\"\n",
    "    # }\n",
    "\n",
    "\n",
    "    device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "    pmodel = models.googlenet(pretrained=True)\n",
    "\n",
    "    num_features = pmodel.fc.in_features\n",
    "\n",
    "    pmodel.fc = Linear(num_features,10)\n",
    "\n",
    "    for name, par in pmodel.named_parameters():\n",
    "        if name not in ['fc.weight', 'fc.bias']:\n",
    "            par.requires_grad = False\n",
    "\n",
    "    total_params = sum(p.numel() for p in pmodel.parameters())\n",
    "    print(f'{total_params:,} total parameters.')\n",
    "    total_trainable_params = sum(\n",
    "        p.numel() for p in pmodel.parameters() if p.requires_grad)\n",
    "    print(f'{total_trainable_params:,} training parameters.')\n",
    "\n",
    "    # for x in pmodel.parameters():\n",
    "    #     x.requires_grad = False\n",
    "    # pmodel.fc.requires_grad = True\n",
    "\n",
    "    \n",
    "    \n",
    "    pmodel = pmodel.to(device)\n",
    "    optimizer = Adam(pmodel.parameters())\n",
    "    loss_function = CrossEntropyLoss()\n",
    "    train_data_loader, validation_data_loader = get_data(param, 'train')\n",
    "\n",
    "    for epo in range(param['epochs']):\n",
    "        totalTrainLoss = 0\n",
    "        totalValLoss = 0\n",
    "        trainCorrect = 0\n",
    "        valCorrect = 0\n",
    "        train_counter=0\n",
    "        validation_counter=0\n",
    "        pmodel.train()\n",
    "        for (image, label) in train_data_loader:\n",
    "            (image, label) = (image.to(device), label.to(device))\n",
    "            prediction = pmodel(image)\n",
    "            loss = loss_function(prediction, label)\n",
    "            ## no optimize.zero_grad() ...\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            totalTrainLoss += loss\n",
    "            trainCorrect += (prediction.argmax(1) == label).type(float).sum().item()\n",
    "            train_counter+=1\n",
    "            # print(train_counter)\n",
    "\n",
    "        pmodel.eval()\n",
    "        with no_grad():\n",
    "            for (image, label) in validation_data_loader:\n",
    "                (image, label) = (image.to(device), label.to(device))\n",
    "                pred = pmodel(image)\n",
    "                loss = loss_function(pred, label)\n",
    "                totalValLoss += loss\n",
    "                valCorrect += (pred.argmax(1) == label).type(float).sum().item()\n",
    "                validation_counter += 1\n",
    "\n",
    "        tr_ls = (totalTrainLoss/train_counter).cpu().detach().numpy()\n",
    "        tr_acc = trainCorrect/len(train_data_loader.dataset)\n",
    "        val_ls = (totalValLoss/validation_counter).cpu().detach().numpy()\n",
    "        val_acc = valCorrect/len(validation_data_loader.dataset)\n",
    "        print(f\"Epoch --> {epo}\")\n",
    "        print(f\"Train Loss --> {tr_ls}\")\n",
    "        print(f\"Train Accuracy --> {tr_acc}\")\n",
    "        print(f\"Validation Loss --> {val_ls}\")\n",
    "        print(f\"Validation Accuracy --> {val_acc}\")\n",
    "        print(\"-----------------------------------------------------------\")\n",
    "        \n",
    "        lg={\n",
    "            'epoch': epo+1,\n",
    "            'tr_accuracy': tr_acc,\n",
    "            'val_accuracy': val_acc,\n",
    "            'tr_loss': tr_ls,\n",
    "            'val_loss': val_ls\n",
    "        }\n",
    "        wandb.log(lg)\n",
    "\n",
    "    # torch.save(model, checkpoint_path)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_config = {\n",
    "  \"method\": \"grid\",  # Use grid search for this example\n",
    "  \"name\": \"PartB VGG16 Sweep\",\n",
    "  \"metric\": {\"goal\": \"maximize\", \"name\": \"val_accuracy\"},\n",
    "  \"parameters\": {\n",
    "    \"batch_size\":{\"values\": [32]},\n",
    "    \"epochs\":{\"values\": [5]},\n",
    "    \"strategy\":{\"values\": ['all_freeze']},  ## K freeze, No Freeze\n",
    "    \"train_data_dir\":{\"values\": [\"./data/train\"]},\n",
    "    \"test_data_dir\":{\"values\": [\"./data/val\"]}\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: 4eidwpsk\n",
      "Sweep URL: https://wandb.ai/cs23m070/cs6910_assignment2/sweeps/4eidwpsk\n"
     ]
    }
   ],
   "source": [
    "# wandb.init()\n",
    "sweep_id = wandb.sweep(sweep_config, project=\"cs6910_assignment2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: epph7rw6 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdense_neurons: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tstrategy: all_freeze\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttest_data_dir: ./data/val\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrain_data_dir: ./data/train\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcs23m070\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.5 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/tirth/Documents/SEM2/DL/assignment/A2/code/cs6910_assignment2/wandb/run-20240403_141655-epph7rw6</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/cs23m070/cs6910_assignment2/runs/epph7rw6' target=\"_blank\">lunar-sweep-1</a></strong> to <a href='https://wandb.ai/cs23m070/cs6910_assignment2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/cs23m070/cs6910_assignment2/sweeps/4eidwpsk' target=\"_blank\">https://wandb.ai/cs23m070/cs6910_assignment2/sweeps/4eidwpsk</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/cs23m070/cs6910_assignment2' target=\"_blank\">https://wandb.ai/cs23m070/cs6910_assignment2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/cs23m070/cs6910_assignment2/sweeps/4eidwpsk' target=\"_blank\">https://wandb.ai/cs23m070/cs6910_assignment2/sweeps/4eidwpsk</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/cs23m070/cs6910_assignment2/runs/epph7rw6' target=\"_blank\">https://wandb.ai/cs23m070/cs6910_assignment2/runs/epph7rw6</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tirth/anaconda3/envs/dl/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/tirth/anaconda3/envs/dl/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=GoogLeNet_Weights.IMAGENET1K_V1`. You can also use `weights=GoogLeNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5,610,154 total parameters.\n",
      "10,250 training parameters.\n",
      "Epoch --> 0\n",
      "Train Loss --> 10.22955322265625\n",
      "Train Accuracy --> 0.271375\n",
      "Validation Loss --> 10.036049842834473\n",
      "Validation Accuracy --> 0.3456728364182091\n",
      "-----------------------------------------------------------\n",
      "Epoch --> 1\n",
      "Train Loss --> 22.519357681274414\n",
      "Train Accuracy --> 0.326375\n",
      "Validation Loss --> 22.084354400634766\n",
      "Validation Accuracy --> 0.3566783391695848\n",
      "-----------------------------------------------------------\n",
      "Epoch --> 2\n",
      "Train Loss --> 24.801694869995117\n",
      "Train Accuracy --> 0.319875\n",
      "Validation Loss --> 20.846763610839844\n",
      "Validation Accuracy --> 0.4112056028014007\n",
      "-----------------------------------------------------------\n",
      "Epoch --> 3\n",
      "Train Loss --> 22.853111267089844\n",
      "Train Accuracy --> 0.477375\n",
      "Validation Loss --> 22.256725311279297\n",
      "Validation Accuracy --> 0.5147573786893447\n",
      "-----------------------------------------------------------\n",
      "Epoch --> 4\n",
      "Train Loss --> 24.960655212402344\n",
      "Train Accuracy --> 0.495\n",
      "Validation Loss --> 20.42535400390625\n",
      "Validation Accuracy --> 0.5327663831915957\n",
      "-----------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▃▅▆█</td></tr><tr><td>tr_accuracy</td><td>▁▃▃▇█</td></tr><tr><td>tr_loss</td><td>▁▇█▇█</td></tr><tr><td>val_accuracy</td><td>▁▁▃▇█</td></tr><tr><td>val_loss</td><td>▁█▇█▇</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>5</td></tr><tr><td>tr_accuracy</td><td>0.495</td></tr><tr><td>tr_loss</td><td>24.96066</td></tr><tr><td>val_accuracy</td><td>0.53277</td></tr><tr><td>val_loss</td><td>20.42535</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">lunar-sweep-1</strong> at: <a href='https://wandb.ai/cs23m070/cs6910_assignment2/runs/epph7rw6' target=\"_blank\">https://wandb.ai/cs23m070/cs6910_assignment2/runs/epph7rw6</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240403_141655-epph7rw6/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.agent(sweep_id, function=train, count=1)\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
