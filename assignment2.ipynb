{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import Module, ReLU, Conv2d, Linear, MaxPool2d, LogSoftmax, NLLLoss, Dropout, BatchNorm2d\n",
    "from torchvision import transforms, datasets\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import flatten, float, no_grad\n",
    "from torch.optim import Adam\n",
    "import torch\n",
    "import wandb\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "PARAMETERS = {\n",
    "    'data_augmentation': False,\n",
    "    'batch_normalization': False,\n",
    "    'filters': 32, # no. of filters in first layer\n",
    "    'filter_org': 'same', # 'half', 'double'\n",
    "    'dropout': 0,\n",
    "    'activation': 'relu',\n",
    "    'train_data_dir': \"./data/train\",\n",
    "    'test_data_dir': \"./data/val\",\n",
    "    'batch_size': 64,\n",
    "    'learning_rate': 0.001,\n",
    "    'epochs': 3,\n",
    "    'dim': 256,\n",
    "    'conv_kernel_size': 3\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASSES = {\n",
    "    0:'Amphibia',\n",
    "    1:'Animalia',\n",
    "    2:'Arachnida',\n",
    "    3:'Aves',\n",
    "    4:'Fungi',\n",
    "    5:'Insecta',\n",
    "    6:'Mammalia',\n",
    "    7:'Mollusca',\n",
    "    8:'Plantae',\n",
    "    9:'Reptilia'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(param, type):\n",
    "    if(type == 'train'):\n",
    "        transform = transforms.Compose([\n",
    "            transforms.RandomHorizontalFlip(p=0.3),\n",
    "            transforms.RandomRotation(degrees=12),\n",
    "            transforms.Resize((param['dim'],param['dim'])),\n",
    "            transforms.ToTensor(), \n",
    "            transforms.Normalize(mean = [0.485, 0.456, 0.406], std = [0.229, 0.224, 0.225])  \n",
    "        ])\n",
    "\n",
    "        tdataset = datasets.ImageFolder(root=param['train_data_dir'], transform=transform)\n",
    "        total = len(tdataset)\n",
    "        train_sample = math.ceil(total*(0.8))\n",
    "        val_sample = total-train_sample\n",
    "        # print(total, train_sample, val_sample)\n",
    "        train_dataset, validation_dataset = torch.utils.data.random_split(tdataset, [train_sample, val_sample])\n",
    "        train_dataloader = DataLoader(train_dataset, batch_size=param['batch_size'], shuffle=True)\n",
    "        validation_dataloader = DataLoader(validation_dataset, batch_size=param['batch_size'], shuffle=False)\n",
    "        return train_dataloader, validation_dataloader\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_data(PARAMETERS, 'train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(Module):\n",
    "    def __init__(self, param):\n",
    "        super(CNN, self).__init__()\n",
    "        self.param=param\n",
    "        self.data_augmentation = param['data_augmentation']\n",
    "        self.batch_normalization = param['batch_normalization']\n",
    "        self.dropout = param['dropout']\n",
    "        self.activation = param['activation']\n",
    "        self.filters = self.filter_logic(param['filters'], param['filter_org'])\n",
    "        self.conv_ks = param['conv_kernel_size']\n",
    "        self.dim = param['dim']\n",
    "        self.dropout = param['dropout']\n",
    "\n",
    "        curr_dim = self.dim\n",
    "        self.conv1 = Conv2d(kernel_size=(self.conv_ks,self.conv_ks), in_channels=3, out_channels=self.filters[0])\n",
    "        curr_dim -= (self.conv_ks-1)\n",
    "        self.act1 = ReLU()\n",
    "        self.pool1 = MaxPool2d(kernel_size=(2, 2), stride=(2, 2))\n",
    "        curr_dim //= 2\n",
    "        self.dropout1 = Dropout(p=self.dropout)\n",
    "\n",
    "        self.conv2 = Conv2d(kernel_size=(self.conv_ks,self.conv_ks), in_channels=self.filters[0], out_channels=self.filters[1])\n",
    "        curr_dim -= (self.conv_ks-1)\n",
    "        self.act2 = ReLU()\n",
    "        self.pool2 = MaxPool2d(kernel_size=(2, 2), stride=(2, 2))\n",
    "        curr_dim //= 2\n",
    "        self.dropout2 = Dropout(p=self.dropout)\n",
    "\n",
    "        self.conv3 = Conv2d(kernel_size=(self.conv_ks,self.conv_ks), in_channels=self.filters[1], out_channels=self.filters[2])\n",
    "        curr_dim -= (self.conv_ks-1)\n",
    "        self.act3 = ReLU()\n",
    "        self.pool3 = MaxPool2d(kernel_size=(2, 2), stride=(2, 2))\n",
    "        curr_dim //= 2\n",
    "        self.dropout3 = Dropout(p=self.dropout)\n",
    "\n",
    "        self.conv4 = Conv2d(kernel_size=(self.conv_ks,self.conv_ks), in_channels=self.filters[2], out_channels=self.filters[3])\n",
    "        curr_dim -= (self.conv_ks-1)\n",
    "        self.act4 = ReLU()\n",
    "        self.pool4 = MaxPool2d(kernel_size=(2, 2), stride=(2, 2))\n",
    "        curr_dim //= 2\n",
    "        self.dropout4 = Dropout(p=self.dropout)\n",
    "\n",
    "        self.conv5 = Conv2d(kernel_size=(self.conv_ks,self.conv_ks), in_channels=self.filters[3], out_channels=self.filters[4])\n",
    "        curr_dim -= (self.conv_ks-1)\n",
    "        self.act5 = ReLU()\n",
    "        self.pool5 = MaxPool2d(kernel_size=(2, 2), stride=(2, 2))\n",
    "        curr_dim //= 2\n",
    "        self.dropout5 = Dropout(p=self.dropout)\n",
    "\n",
    "        self.fc1 = Linear(in_features=(curr_dim * curr_dim * self.filters[4]), out_features=(curr_dim * curr_dim * self.filters[4]))  # How to calculate dimension of filters at previous level\n",
    "        self.act6 = ReLU()\n",
    "        self.dropout6 = Dropout(p=0.5)\n",
    "        \n",
    "        self.out = Linear(in_features=(curr_dim * curr_dim * self.filters[4]), out_features=10)\n",
    "        self.act7 = LogSoftmax(dim=1)\n",
    "\n",
    "    \n",
    "    def filter_logic(self, filter, org):\n",
    "        level = []\n",
    "        org = org.lower()\n",
    "        if org == 'same':\n",
    "            level = [filter for i in range(5)]\n",
    "        elif org == 'double':\n",
    "            level = [filter*pow(2,i) for i in range(5)]\n",
    "        elif org == 'half':\n",
    "            level = [max(filter//pow(2,i),1) for i in range(5)]\n",
    "        return level\n",
    "\n",
    "    \n",
    "\n",
    "    def forward(self, r):\n",
    "\n",
    "        r=self.conv1(r)\n",
    "        r=self.act1(r)\n",
    "        r=self.pool1(r)\n",
    "        r=self.dropout1(r)\n",
    "\n",
    "        r=self.conv2(r)\n",
    "        r=self.act2(r)\n",
    "        r=self.pool2(r)\n",
    "        r=self.dropout2(r)\n",
    "\n",
    "        r=self.conv3(r)\n",
    "        r=self.act3(r)\n",
    "        r=self.pool3(r)\n",
    "        r=self.dropout3(r)\n",
    "\n",
    "        r=self.conv4(r)\n",
    "        r=self.act4(r)\n",
    "        r=self.pool4(r)\n",
    "        r=self.dropout4(r)\n",
    "\n",
    "        r=self.conv5(r)\n",
    "        r=self.act5(r)\n",
    "        r=self.pool5(r)\n",
    "        r=self.dropout5(r)\n",
    "\n",
    "        r=flatten(r,1)\n",
    "        r=self.fc1(r)\n",
    "        r=self.act6(r)\n",
    "        r=self.dropout6(r)\n",
    "        \n",
    "        r=self.out(r)\n",
    "        output=self.act7(r)\n",
    "\n",
    "        return output\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train(param):\n",
    "#     device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "#     model = CNN(param).to(device)\n",
    "#     optimizer = Adam(model.parameters(), lr=param['learning_rate'])\n",
    "#     loss_function = NLLLoss()\n",
    "#     train_data_loader, validation_data_loader = get_data(param, 'train')\n",
    "    \n",
    "\n",
    "#     for _ in range(param['epochs']):\n",
    "#         model.train()\n",
    "#         totalTrainLoss = 0\n",
    "#         totalValLoss = 0\n",
    "#         trainCorrect = 0\n",
    "#         valCorrect = 0\n",
    "#         train_counter=0\n",
    "#         validation_counter=0\n",
    "#         for (image, label) in train_data_loader:\n",
    "#             (image, label) = (image.to(device), label.to(device))\n",
    "#             prediction = model(image)\n",
    "#             loss = loss_function(prediction, label)\n",
    "            \n",
    "#             optimizer.zero_grad()\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "            \n",
    "#             totalTrainLoss += loss\n",
    "#             trainCorrect += (prediction.argmax(1) == label).type(float).sum().item()\n",
    "#             train_counter+=1\n",
    "#             print(train_counter)\n",
    "        \n",
    "#         with no_grad():\n",
    "#             model.eval()\n",
    "#             for (image, label) in validation_data_loader:\n",
    "#                 (image, label) = (image.to(device), label.to(device))\n",
    "#                 pred = model(image)\n",
    "#                 totalValLoss += loss_function(pred, label)\n",
    "#                 valCorrect += (pred.argmax(1) == label).type(float).sum().item()\n",
    "#                 validation_counter+=1\n",
    "\n",
    "#         print(f\"Train Loss --> {(totalTrainLoss/train_counter).cpu().detach().numpy()}\")\n",
    "#         print(f\"Train Accuracy --> {trainCorrect/len(train_data_loader.dataset)}\")\n",
    "#         print(f\"Validation Loss --> {(totalValLoss/validation_counter).cpu().detach().numpy()}\")\n",
    "#         print(f\"Validation Accuracy --> {valCorrect/len(validation_data_loader.dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train(PARAMETERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## for wandb sweeps\n",
    "def train():\n",
    "    wandb.init()\n",
    "    param = wandb.config\n",
    "    wandb.run.name = f'fltr_{param.filters}_fltrOrg_{param.filter_org}_dataAug_{param.data_augmentation}_batchNorm_{param.batch_normalization}_act_{param.activation}_batchSz_{param.batch_size}'\n",
    "\n",
    "    device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "    model = CNN(param).to(device)\n",
    "    optimizer = Adam(model.parameters(), lr=param['learning_rate'])\n",
    "    loss_function = NLLLoss()\n",
    "    train_data_loader, validation_data_loader = get_data(param, 'train')\n",
    "    \n",
    "\n",
    "    for epo in range(param['epochs']):\n",
    "        model.train()\n",
    "        totalTrainLoss = 0\n",
    "        totalValLoss = 0\n",
    "        trainCorrect = 0\n",
    "        valCorrect = 0\n",
    "        train_counter=0\n",
    "        validation_counter=0\n",
    "        for (image, label) in train_data_loader:\n",
    "            (image, label) = (image.to(device), label.to(device))\n",
    "            prediction = model(image)\n",
    "            loss = loss_function(prediction, label)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            totalTrainLoss += loss\n",
    "            trainCorrect += (prediction.argmax(1) == label).type(float).sum().item()\n",
    "            train_counter+=1\n",
    "            print(train_counter)\n",
    "        \n",
    "        with no_grad():\n",
    "            model.eval()\n",
    "            for (image, label) in validation_data_loader:\n",
    "                (image, label) = (image.to(device), label.to(device))\n",
    "                pred = model(image)\n",
    "                totalValLoss += loss_function(pred, label)\n",
    "                valCorrect += (pred.argmax(1) == label).type(float).sum().item()\n",
    "                validation_counter+=1\n",
    "\n",
    "        tr_ls = (totalTrainLoss/train_counter).cpu().detach().numpy()\n",
    "        tr_acc = trainCorrect/len(train_data_loader.dataset)\n",
    "        val_ls = (totalValLoss/validation_counter).cpu().detach().numpy()\n",
    "        val_acc = valCorrect/len(validation_data_loader.dataset)\n",
    "        print(f\"Train Loss --> {tr_ls}\")\n",
    "        print(f\"Train Accuracy --> {tr_acc}\")\n",
    "        print(f\"Validation Loss --> {val_ls}\")\n",
    "        print(f\"Validation Accuracy --> {val_acc}\")\n",
    "        \n",
    "        lg={\n",
    "            'epoch': epo+1,\n",
    "            'tr_accuracy': tr_acc,\n",
    "            'val_accuracy': val_acc,\n",
    "            'tr_loss': tr_ls,\n",
    "            'val_loss': val_ls\n",
    "        }\n",
    "        wandb.log(lg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_config = {\n",
    "  \"method\": \"grid\",  # Use grid search for this example\n",
    "  \"name\": \"Q1 Sweep\",\n",
    "  \"metric\": {\"goal\": \"maximize\", \"name\": \"val_accuracy\"},\n",
    "  \"parameters\": {\n",
    "    \"data_augmentation\":{\"values\": [True]},  # List of boolean values\n",
    "    \"batch_normalization\":{\"values\": [False]},  # List of boolean values\n",
    "    \"filters\":{\"values\": [64]},  # List of filter values for first layer\n",
    "    \"filter_org\":{\"values\": [\"half\"]},  # List of filter organization options.. , \"half\", \"double\"\n",
    "    \"dropout\":{\"values\": [0.2]},  # Dropout rates\n",
    "    \"activation\":{\"values\": [\"relu\"]},  # Activation functions... , \"tanh\", \"leaky_relu\"\n",
    "    \"batch_size\":{\"values\": [64]},\n",
    "    \"learning_rate\":{\"values\": [0.001]},\n",
    "    \"epochs\":{\"values\": [10]},\n",
    "    \"dim\":{\"values\": [256]},\n",
    "    \"conv_kernel_size\":{\"values\": [3]},\n",
    "    \"train_data_dir\":{\"values\": [\"./data/train\"]},\n",
    "    \"test_data_dir\":{\"values\": [\"./data/val\"]}\n",
    "  }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: 468ahafa\n",
      "Sweep URL: https://wandb.ai/cs23m070/cs6910_assignment2/sweeps/468ahafa\n"
     ]
    }
   ],
   "source": [
    "# wandb.init()\n",
    "sweep_id = wandb.sweep(sweep_config, project=\"cs6910_assignment2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: iooz0ffa with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: relu\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_normalization: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tconv_kernel_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_augmentation: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdim: 256\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_org: half\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfilters: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttest_data_dir: ./data/val\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrain_data_dir: ./data/train\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcs23m070\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.5 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/tirth/Documents/SEM2/DL/assignment/A2/code/cs6910_assignment2/wandb/run-20240401_122724-iooz0ffa</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/cs23m070/cs6910_assignment2/runs/iooz0ffa' target=\"_blank\">exalted-sweep-1</a></strong> to <a href='https://wandb.ai/cs23m070/cs6910_assignment2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/cs23m070/cs6910_assignment2/sweeps/468ahafa' target=\"_blank\">https://wandb.ai/cs23m070/cs6910_assignment2/sweeps/468ahafa</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/cs23m070/cs6910_assignment2' target=\"_blank\">https://wandb.ai/cs23m070/cs6910_assignment2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/cs23m070/cs6910_assignment2/sweeps/468ahafa' target=\"_blank\">https://wandb.ai/cs23m070/cs6910_assignment2/sweeps/468ahafa</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/cs23m070/cs6910_assignment2/runs/iooz0ffa' target=\"_blank\">https://wandb.ai/cs23m070/cs6910_assignment2/runs/iooz0ffa</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "Train Loss --> 2.284712553024292\n",
      "Train Accuracy --> 0.123625\n",
      "Validation Loss --> 2.2659122943878174\n",
      "Validation Accuracy --> 0.17508754377188596\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "Train Loss --> 2.2340598106384277\n",
      "Train Accuracy --> 0.16775\n",
      "Validation Loss --> 2.224869966506958\n",
      "Validation Accuracy --> 0.1950975487743872\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "Train Loss --> 2.2000861167907715\n",
      "Train Accuracy --> 0.189625\n",
      "Validation Loss --> 2.1821486949920654\n",
      "Validation Accuracy --> 0.21810905452726362\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "Train Loss --> 2.181879997253418\n",
      "Train Accuracy --> 0.207875\n",
      "Validation Loss --> 2.214125156402588\n",
      "Validation Accuracy --> 0.2036018009004502\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "Train Loss --> 2.145817279815674\n",
      "Train Accuracy --> 0.227375\n",
      "Validation Loss --> 2.129163980484009\n",
      "Validation Accuracy --> 0.25162581290645325\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "Train Loss --> 2.1178178787231445\n",
      "Train Accuracy --> 0.243\n",
      "Validation Loss --> 2.1115968227386475\n",
      "Validation Accuracy --> 0.25012506253126565\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "Train Loss --> 2.085479497909546\n",
      "Train Accuracy --> 0.247375\n",
      "Validation Loss --> 2.0799152851104736\n",
      "Validation Accuracy --> 0.2671335667833917\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "Train Loss --> 2.0595476627349854\n",
      "Train Accuracy --> 0.255625\n",
      "Validation Loss --> 2.0421547889709473\n",
      "Validation Accuracy --> 0.26863431715857927\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "Train Loss --> 2.0565593242645264\n",
      "Train Accuracy --> 0.266125\n",
      "Validation Loss --> 2.065808057785034\n",
      "Validation Accuracy --> 0.27963981990995496\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "Train Loss --> 2.028323173522949\n",
      "Train Accuracy --> 0.2715\n",
      "Validation Loss --> 2.016453266143799\n",
      "Validation Accuracy --> 0.2926463231615808\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>tr_accuracy</td><td>▁▃▄▅▆▇▇▇██</td></tr><tr><td>tr_loss</td><td>█▇▆▅▄▃▃▂▂▁</td></tr><tr><td>val_accuracy</td><td>▁▂▄▃▆▅▆▇▇█</td></tr><tr><td>val_loss</td><td>█▇▆▇▄▄▃▂▂▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>10</td></tr><tr><td>tr_accuracy</td><td>0.2715</td></tr><tr><td>tr_loss</td><td>2.02832</td></tr><tr><td>val_accuracy</td><td>0.29265</td></tr><tr><td>val_loss</td><td>2.01645</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">exalted-sweep-1</strong> at: <a href='https://wandb.ai/cs23m070/cs6910_assignment2/runs/iooz0ffa' target=\"_blank\">https://wandb.ai/cs23m070/cs6910_assignment2/runs/iooz0ffa</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240401_122724-iooz0ffa/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.agent(sweep_id, function=train, count=1)\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.], device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "# import torch\n",
    "# if torch.backends.mps.is_available():\n",
    "#     mps_device = torch.device(\"mps\")\n",
    "#     x = torch.ones(1, device=mps_device)\n",
    "#     print (x)\n",
    "# else:\n",
    "#     print (\"MPS device not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps\n"
     ]
    }
   ],
   "source": [
    "# device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "# print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
